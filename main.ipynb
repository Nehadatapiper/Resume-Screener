{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "from PyPDF2 import PdfReader\n",
    "import docx2txt\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained spaCy BERT model from the specified path\n",
    "nlp_bert = spacy.load('/home/neha/workspace/neha--BERT/BERT_model_test/Fine_tune_BERT_with_spacy3/model-best')\n",
    "\n",
    "# initializing the countvectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Initializing the Sentence Transformer model using BERT with mean-tokens pooling for embeddings creation\n",
    "model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the from the text extracted\n",
    "def get_text_labels(text):\n",
    "    doc_bert = nlp_bert(text)\n",
    "    text_corpus = []\n",
    "    label_corpus = []\n",
    "    # Iterate through the named entities (entities) recognized by the model\n",
    "    for ent in doc_bert.ents:\n",
    "    # Print the recognized text and its corresponding label\n",
    "    #   print(ent.text, \"  ->>>>  \", ent.label_)\n",
    "        text_corpus.append(ent.text)\n",
    "        label_corpus.append(ent.label_)\n",
    "    # print(\"/nPrinting the length of skill words in the resume/n\")\n",
    "    # print(len(text_corpus))    \n",
    "\n",
    "    # cleaning the text_corpus    \n",
    "    text_corpus_cleaned = []\n",
    "    for sub in text_corpus:\n",
    "        text_corpus_cleaned.append(sub.replace(\"\\n\",\"\"))\n",
    "    # print(text_corpus_cleaned)\n",
    "    # print(\"\\n\")\n",
    "\n",
    "\n",
    "    required_text = []\n",
    "    for ele in range(len(label_corpus)):\n",
    "        if label_corpus[ele]=='DEGREE' or label_corpus[ele]=='SKILLS' or label_corpus=='DIPLOMA_MAJOR' or label_corpus=='EXPERIENCE':\n",
    "                required_text.append(text_corpus_cleaned[ele])\n",
    "   # print(required_text)\n",
    "            \n",
    "    # converting the text_corpus to string\n",
    "    skills_corpus = ' '.join(map(str, required_text))\n",
    "    return skills_corpus        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from docx files\n",
    "def extract_text_from_docx(docx_path):\n",
    "    text = docx2txt.process(docx_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to extract text from pdf files\n",
    "def extract_text_from_pdf(pdf_file_path):\n",
    "    reader = PdfReader(pdf_file_path)\n",
    "    number_of_pages = len(reader.pages)\n",
    "    page = reader.pages[0]\n",
    "    text = page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_text_from_txt(txt_file_path):\n",
    "#     with open(txt_file_path) as f:\n",
    "#         contents = f.read()\n",
    "#     return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to check the file type and calls the extract function to get the text from various resume files\n",
    "def check_file_type(name_of_file):# considered two file formats - 1.pdf , 2.docx\n",
    "    if name_of_file.endswith('.pdf'):\n",
    "        text = extract_text_from_pdf(name_of_file)\n",
    "    elif name_of_file.endswith('.docx'):\n",
    "        text = extract_text_from_docx(name_of_file)\n",
    "    # elif name_of_file.endswith('.txt'):\n",
    "    #     text  = extract_text_from_txt(name_of_file)\n",
    "    skills_extracted = get_text_labels(text)\n",
    "\n",
    "    return skills_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing the punctuations ,stopswords and wordlematizer\n",
    "punctuations_ = string.punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('https')\n",
    "lematizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text preprocessing for the job_description\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "\n",
    "def lematize_words(text):\n",
    "    return \" \".join([lematizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = text.lower()\n",
    "    for words in punctuations_:\n",
    "        text = text.replace(words,\"\")\n",
    "    text = \" \".join(text.split('\\n'))\n",
    "    text = remove_stopwords(text)\n",
    "    text  = lematize_words(text)\n",
    "    text = re.sub(' +',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the matching score    \n",
    "def score_calculation_cv(skills_corpus,jd_require):\n",
    "    test_pair = [skills_corpus,jd_require]\n",
    "    count_matrix = cv.fit_transform(test_pair)\n",
    "    # print(\"\\nSimilarity Scores:\")\n",
    "    # print(cosine_similarity(count_matrix))\n",
    "    # print(\"--------------------------------------\")\n",
    "    # print(\"--------------------------------------\")\n",
    "    matchPercentage = cosine_similarity(count_matrix)[0][1] * 100\n",
    "    matchPercentage = round(matchPercentage, 2) # round to two decimal\n",
    "    return matchPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_calculation_bert(skills_corpus,jd_require):\n",
    "       \n",
    "        # Encoding the sentences to obtain their embeddings\n",
    "        skills_embeddings = model.encode(skills_corpus)\n",
    "        jd_embeddings  = model.encode(jd_require)\n",
    "\n",
    "        # Calculating the cosine similarity between the first sentence embedding and the rest of the embeddings\n",
    "        # The result will be a list of similarity scores between the first sentence and each of the other sentences\n",
    "        similarity_scores = cosine_similarity(skills_embeddings, jd_embeddings)\n",
    "        # print(similarity_scores)\n",
    "        matchPercentage = similarity_scores[0][0]*100\n",
    "        matchPercentage = round(matchPercentage,2)\n",
    "        return matchPercentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/neha/workspace/Resume_selector/job_requirements.txt') as f:\n",
    "    contents = f.read()\n",
    "jd_require = text_cleaning(contents)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resume_rank(resume_names,result_cv,result_bert):\n",
    "\n",
    "    dict_cv= {\"Name\":resume_names,\"Score_CV\":result_cv}\n",
    "    df1  = pd.DataFrame(dict_cv)\n",
    "    dict_bert = {\"Name\":resume_names,\"Score_BERT\":result_bert}\n",
    "    df2 = pd.DataFrame(dict_bert)\n",
    "    test1 = df1.sort_values('Score', ascending=False)\n",
    "    test2 = df2.sort_values('Score',ascending=False)\n",
    "    print(\"\\nDisplaying the rank using Countvectorizer\\n\")\n",
    "    print(test1)\n",
    "    print(\"\\nDisplaying the result using BERT\\n\")\n",
    "    print(test2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying the rank using Countvectorizer\n",
      "\n",
      "                            Name  Score\n",
      "0                    Hasibul.pdf   6.00\n",
      "1                  PRITI PAL.pdf   5.30\n",
      "2            Dipakshi Kumari.pdf   4.35\n",
      "4  John Benedict L. Soledad .pdf   1.23\n",
      "3                  Cara Chiu.pdf   0.00\n",
      "\n",
      "Displaying the result using BERT\n",
      "\n",
      "                            Name  Score\n",
      "0                    Hasibul.pdf  61.49\n",
      "4  John Benedict L. Soledad .pdf  58.20\n",
      "1                  PRITI PAL.pdf  57.68\n",
      "2            Dipakshi Kumari.pdf  54.84\n",
      "3                  Cara Chiu.pdf  49.16\n"
     ]
    }
   ],
   "source": [
    "result_cv = []\n",
    "result_bert = []\n",
    "resume_names = []\n",
    "\n",
    "test_folder_path = r\"/home/neha/workspace/Final/test/\"\n",
    "for file in os.listdir(test_folder_path):\n",
    "    # print(file)\n",
    "    resume_names.append(file)\n",
    "    resume_file_path = test_folder_path+file\n",
    "    skills_corpus = check_file_type(resume_file_path)\n",
    "    skills_corpus = text_cleaning(skills_corpus)\n",
    "    score1 = score_calculation_cv(skills_corpus,jd_require)\n",
    "    score2 = score_calculation_bert([skills_corpus],[jd_require])\n",
    "    result_cv.append(score1)\n",
    "    result_bert.append(score2)\n",
    "resume_rank(resume_names,result_cv,result_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_bert[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for value in result:\n",
    "#     print(value)\n",
    "#     if str(value)=='None':\n",
    "#         result.remove(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict= {\"Name\":resume_names,\"Score\":result}\n",
    "# df  = pd.DataFrame(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = df.sort_values('Score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wslenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
